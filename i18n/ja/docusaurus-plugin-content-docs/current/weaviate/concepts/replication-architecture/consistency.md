---
title: 一貫性
sidebar_position: 4
description: " Weaviate クラスター内のレプリカ間でのレプリケーションファクター設定とデータ整合性モデル。"
image: og/docs/concepts.jpg
# tags: ['architecture']
---

import SkipLink from '/src/components/SkipValidationLink'

 Weaviate におけるレプリケーションファクターは、シャード（レプリカとも呼ばれます）のコピーをクラスター全体にいくつ保持するかを決定します。

<p align="center"><img src="/img/docs/replication-architecture/replication-factor.png" alt="Replication factor" width="80%"/></p>

レプリケーションファクターが  > 1 になると、整合性モデルはシステムの信頼性、スケーラビリティ、パフォーマンス要件のバランスを取ります。

 Weaviate は複数の整合性モデルを使用します。クラスター メタデータ用とデータオブジェクト用で、それぞれ異なるモデルです。

### Weaviate における整合性モデル

 Weaviate は [Raft](https://raft.github.io/) コンセンサスアルゴリズムを使用して [クラスター メタデータのレプリケーション](./cluster-architecture.md#metadata-replication-raft) を行います。ここでのクラスター メタデータには、コレクション定義やテナントのアクティビティ ステータスが含まれます。これにより、一部ノードがダウンしていてもメタデータを更新できます。

データオブジェクトは、[リーダーレス設計](./cluster-architecture.md#data-replication-leaderless) と調整可能な整合性レベルを用いてレプリケートされます。そのため、データ操作をより一貫性重視にも可用性重視にも調整でき、望ましいトレードオフを実現できます。

これらの設計は、[CAP 定理](./index.md#cap-theorem) で説明される整合性と可用性のトレードオフを反映しています。

:::tip 整合性に関する経験則
整合性の強さは、次の条件で判断できます。
* r + w  > n の場合、システムは強い整合性を持ちます。  
    * r は読み取り操作の整合性レベル  
    * w は書き込み操作の整合性レベル  
    * n はレプリケーションファクター（レプリカ数）
* r + w &lt;= n の場合、このシナリオで到達できるのは最終的な整合性です。
:::

## クラスター メタデータ

クラスター メタデータは Raft アルゴリズムを使用します。

 `v1.25` 以降、 Weaviate はクラスター メタデータのレプリケーションに [Raft](https://raft.github.io/) コンセンサスアルゴリズムを採用しています。Raft はリーダー選出型のコンセンサス アルゴリズムで、ログベースの方式によりクラスター全体へレプリケーションを調整します。

その結果、クラスター メタデータを変更するリクエストはすべてリーダーノードへ送信されます。リーダーノードは自分のログに変更を適用した後、フォロワーノードへ変更を伝播します。ノードの過半数が変更を承認すると、リーダーノードが変更をコミットし、クライアントへ確定を返します。

このアーキテクチャにより、少数のノード障害が発生してもクラスター全体でメタデータの整合性が保たれます。

<details>
  <summary><code>v1.25</code> 以前のクラスター メタデータ コンセンサス アルゴリズム</summary>

Raft を導入する前は、クラスター メタデータの更新は [分散トランザクション](https://en.wikipedia.org/wiki/Distributed_transaction) アルゴリズムで実施していました。これは分散ネットワーク上の複数ノードのデータベースを横断して一連の操作を行う手法です。 Weaviate では [2 フェーズコミット (2PC)](https://en.wikipedia.org/wiki/Two-phase_commit_protocol) プロトコルを使用し、ミリ秒単位でクラスター メタデータをレプリケートしていました。

フェイルがない正常実行では 2 つのフェーズがあります。  
1. コミット要求フェーズ（投票フェーズ）: コーディネーターノードが各ノードへ更新を受け取り処理できるかを確認します。  
2. コミットフェーズ: コーディネーターノードが各ノードへ変更をコミットします。

</details>

### クエリにおけるコレクション定義の取得

:::info `v1.27.10`, `v1.28.4` で追加
:::

一部のクエリではコレクション定義が必要です。この機能導入前は、そのようなクエリのたびにローカルノードがリーダーノードから定義を取得していました。これにより整合性は強く保たれますが、トラフィックと負荷が増加する可能性がありました。

対応している場合、`COLLECTION_RETRIEVAL_STRATEGY` [環境変数](/deploy/configuration/env-vars/index.md#multi-node-instances) に `LeaderOnly`、`LocalOnly`、`LeaderOnMismatch` を設定できます。

- `LeaderOnly` (デフォルト): 常にリーダーノードから定義を取得します。最も整合性が高い一方、クラスター内部トラフィックが増える可能性があります。
- `LocalOnly`: 常にローカルの定義を使用します。最終的な整合性となりますが、クラスター内部トラフィックを削減できます。
- `LeaderOnMismatch`: ローカル定義が古いかをチェックし、必要に応じてリーダーから取得します。整合性とトラフィックのバランスを取ります。

デフォルトは強い整合性を得るため `LeaderOnly` です。ただし、`LocalOnly` や `LeaderOnMismatch` を用いることで、望ましい整合性レベルに応じてトラフィックを削減できます。

## データオブジェクト

 Weaviate はデータオブジェクトに対して、整合性レベルに応じて調整された 2 フェーズコミットを使用します。たとえば `QUORUM` 書き込み（後述）の場合、5 ノードが存在すると 3 件のリクエストが送信され、それぞれが内部で 2 フェーズコミットを実行します。

その結果、 Weaviate のデータオブジェクトは最終的に整合性が取られます。最終的な整合性は BASE セマンティクスを提供します。

* **Basically available**: 読み取りおよび書き込み操作は可能な限り利用可能  
* **Soft-state**: 更新がまだ収束していないため整合性保証はありません  
* **Eventually consistent**: システムが十分に長く稼働し書き込みが行われた後、すべてのノードが整合性を持つようになります  

 Weaviate は可用性向上のため最終的な整合性を採用しています。読み取りと書き込みの整合性は調整可能で、アプリケーション要件に合わせて可用性と整合性のトレードオフを選択できます。

*下のアニメーションは、レプリケーションファクター 3、ノード数 8 の環境で `QUORUM` 整合性を設定した書き込みまたは読み取りの例です。青色のノードがコーディネーターノードとして動作します。`QUORUM` が設定されているため、コーディネーターノードは 3 件中 2 件のレスポンスを受け取った時点でクライアントへ結果を返します。*

<p align="center"><img src="/img/docs/replication-architecture/replication-quorum-animation.gif" alt="Write consistency QUORUM" width="75%"/></p>

### 書き込み整合性の調整

データオブジェクトの追加または変更は **書き込み** 操作です。

:::note
書き込み操作の整合性は、 Weaviate  v1.18 から `ONE`、`QUORUM`（デフォルト）、`ALL` に調整できます。 v1.17 では書き込み整合性は常に `ALL`（最高整合性）です。
:::

書き込み整合性を設定可能にした主な理由は、 v1.18 で自動修復が導入されたためです。書き込みは選択した整合性レベルにかかわらず n（レプリケーションファクター） ノードに対して必ず行われます。ただしコーディネーターノードが待機する ACK 数が `ONE`、`QUORUM`、`ALL` で異なります。読み取りリクエストでの修復機能がない状況で、すべてのノードに確実に書き込みを適用するため、 v1.17 までは書き込み整合性が `ALL` に固定されていました。 v1.18 以降の設定は以下のとおりです:
* **ONE** - 少なくとも 1 つのレプリカから ACK を受け取れば書き込み完了とみなします。最も高速（高可用性）ですが、整合性は最低です。
* **QUORUM** - 少なくとも `QUORUM` レプリカから ACK を受け取る必要があります。`QUORUM` は _n / 2 + 1_ で計算されます（_n_ はレプリカ数）。例: レプリケーションファクター 6 ではクォーラムは 4 で、2 レプリカのダウンに耐えられます。
* **ALL** - すべてのレプリカから ACK を受け取る必要があります。最も整合性が高いですが、最も遅く（低可用性）な選択肢です。

*下図: レプリケーションファクター 3、ノード数 8、書き込み整合性 `ONE` の Weaviate レプリケート構成。*

<p align="center"><img src="/img/docs/replication-architecture/replication-rf3-c-ONE.png" alt="Write consistency ONE" width="60%"/></p>

*下図: レプリケーションファクター 3、ノード数 8、書き込み整合性 `QUORUM`（n/2+1）の構成。*

<p align="center"><img src="/img/docs/replication-architecture/replication-rf3-c-QUORUM.png" alt="Write consistency QUORUM" width="60%"/></p>

*下図: レプリケーションファクター 3、ノード数 8、書き込み整合性 `ALL` の構成。*

<p align="center"><img src="/img/docs/replication-architecture/replication-rf3-c-ALL.png" alt="Write consistency ALL" width="60%"/></p>

### 読み取り整合性の調整

読み取り操作は Weaviate のデータオブジェクトに対する GET リクエストです。書き込みと同様、読み取り整合性も `ONE`、`QUORUM`（デフォルト）、`ALL` に調整可能です。

:::note
`v1.18` 以前は、[ID でオブジェクトを取得するリクエスト](../../manage-objects/read.mdx#get-an-object-by-id) にのみ整合性レベルを設定でき、それ以外の読み取りリクエストはすべて `ALL` でした。
:::

以下の整合性レベルはほとんどの読み取り操作に適用されます。

- `v1.18` 以降、REST エンドポイントで整合性レベルを指定できます。  
- `v1.19` 以降、GraphQL `Get` リクエストでも整合性レベルを指定できます。  
- すべての gRPC ベースの読み書き操作は調整可能な整合性レベルをサポートします。

* **ONE** - 少なくとも 1 つのレプリカからレスポンスが得られれば読み取り完了とみなします。最も高速（高可用性）ですが、整合性は最低です。
* **QUORUM** - `QUORUM` 数のレプリカからレスポンスが必要です。`QUORUM` は _n / 2 + 1_ で計算されます。例: レプリケーションファクター 6 ではクォーラムは 4 で、2 レプリカのダウンに耐えられます。
* **ALL** - すべてのレプリカからレスポンスを受け取る必要があります。少なくとも 1 つのレプリカが応答しないと読み取りは失敗します。最も整合性が高いですが、最も遅い（低可用性）選択肢です。

例:
* **ONE**  
  単一データセンターでレプリケーションファクター 3、読み取り整合性 `ONE` の場合、コーディネーターノードは 1 つのレプリカからのレスポンスを待ちます。

  <p align="center"><img src="/img/docs/replication-architecture/replication-rf3-c-ONE.png" alt="Write consistency ONE" width="60%"/></p>

* **QUORUM**  
  単一データセンターでレプリケーションファクター 3、読み取り整合性 `QUORUM` の場合、コーディネーターノードは n / 2 + 1 = 3 / 2 + 1 = 2 つのレプリカからレスポンスを待ちます。

  <p align="center"><img src="/img/docs/replication-architecture/replication-rf3-c-QUORUM.png" alt="Write consistency QUORUM" width="60%"/></p>

* **ALL**  
  単一データセンターでレプリケーションファクター 3、読み取り整合性 `ALL` の場合、コーディネーターノードは 3 つすべてのレプリカからレスポンスを待ちます。

  <p align="center"><img src="/img/docs/replication-architecture/replication-rf3-c-ALL.png" alt="Write consistency ALL" width="60%"/></p>
### 調整可能な一貫性戦略

一貫性と速度のトレードオフに応じて、以下に書き込み / 読み取り操作の一般的な一貫性レベルの組み合わせを示します。これらは最終的な一貫性を保証するための _最小_ 要件です:  
* `QUORUM` / `QUORUM` => 書き込みと読み取りのレイテンシがバランス  
* `ONE` / `ALL` => 書き込みが高速、読み取りが低速 (書き込み最適化)  
* `ALL` / `ONE` => 書き込みが低速、読み取りが高速 (読み取り最適化)

### 調整可能な一貫性とクエリ

読み取り操作における調整可能な一貫性レベルは、クエリで返されるオブジェクト一覧の一貫性には影響しません。言い換えると、クエリで返されるオブジェクトの UUID 一覧は、コーディネーターノード (および必要に応じたその他のシャード) のローカルインデックスのみに依存し、読み取り一貫性レベルとは無関係です。

これは、各クエリがコーディネーターノードと、クエリを解決するために必要な他のシャードによって実行されるためです。読み取り一貫性レベルを `ALL` に設定しても、複数のレプリカにクエリを発行して結果をマージするわけではありません。

読み取り一貫性レベルが適用されるのは、特定されたオブジェクトをレプリカから取得する際です。たとえば、読み取り一貫性レベルを `ALL` に設定すると、コーディネーターノードはすべてのレプリカからオブジェクトが返されるのを待ちます。`ONE` に設定した場合、コーディネーターノードは自身だけからオブジェクトを返すことがあります。

つまり、読み取り一貫性レベルは取得されるオブジェクトのバージョンには影響しますが、クエリ結果をより (あるいはより少なく) 一貫性のあるものにするわけではありません。

:::note いつ発生する可能性がありますか?
デフォルトでは、Weaviate は挿入 / 更新 / 削除時にすべてのノードへ書き込みを行います。そのため、ほとんどの場合すべてのシャードは同一のローカルインデックスを持ち、この問題は発生しません。これはノードがダウンしている、ネットワークに問題があるなどの稀なケースでのみ発生します。
:::

### テナント状態とデータオブジェクト

[マルチテナントコレクション](../data.md#multi-tenancy) では、各テナントに設定可能な [テナント状態](../../starter-guides/managing-resources/tenant-states.mdx) があり、データの可用性と配置を決定します。テナント状態は `active`, `inactive`, `offloaded` に設定できます。

`active` テナントのデータはクエリと更新が可能であるはずですが、`inactive` や `offloaded` テナントはそうではありません。

しかし、テナント状態が変更されてから、データがその (宣言的な) テナント状態を反映するまでに遅延が発生する場合があります。

その結果、テナント状態が `inactive` や `offloaded` に設定されていても、一定期間データがクエリ可能である場合があります。逆に、`active` に設定されていても、一定期間データがクエリや更新に利用できない場合があります。

:::info なぜ repair-on-read で解決されないのですか?
速度を優先するため、テナントに対するデータ操作はテナントのアクティビティ状態操作とは独立して行われます。そのため、テナント状態は repair-on-read 操作では更新されません。
:::

## 修復

Weaviate のような分散システムでは、ネットワーク問題、ノード障害、タイミング競合などによりオブジェクトレプリカが不整合になることがあります。Weaviate はレプリカ間で不整合を検出すると、同期していないデータを修復しようとします。

Weaviate は [非同期レプリケーション](#async-replication)、[削除解決](#deletion-resolution-strategies)、[repair-on-read](#repair-on-read) の各戦略を用いてレプリカ間の一貫性を維持します。

### 非同期レプリケーション

:::info `v1.26` で追加
:::

非同期レプリケーションは、同じデータを保持するノード間で最終的な一貫性を確保する Weaviate のバックグラウンド同期プロセスです。各シャードが複数ノードにレプリケートされる場合、非同期レプリケーションは定期的にデータを比較・伝播し、すべてのノードが同期を保つよう保証します。

このプロセスでは Merkle ツリー (ハッシュツリー) アルゴリズムを使用してクラスタ内ノードの状態を監視・比較します。不整合が検出されると、対象ノードのデータを再同期します。

repair-on-read は 1 つまたは 2 つの孤立した修復には効果的ですが、非同期レプリケーションは多数の不整合が存在する状況で効果を発揮します。たとえばオフラインノードが複数の更新を逃した場合、ノードが復旧すると非同期レプリケーションが迅速に一貫性を回復します。

非同期レプリケーションは repair-on-read を補完します。同期チェックの合間にノードが不整合になった場合、repair-on-read が読み取り時に問題を検知します。

非同期レプリケーションを有効化するには、コレクション定義の [`replicationConfig` セクション](../../manage-collections/multi-node-setup.mdx#replication-settings) で `asyncEnabled` を true に設定します。利用可能な設定の詳細は [How-to: Replication](/deploy/configuration/replication.md#async-replication-settings) を参照してください。

#### 非同期レプリケーションのメモリおよびパフォーマンスの考慮事項

:::info `v1.29` で追加
:::

非同期レプリケーションは、オブジェクトの最新更新時間に基づき、ハッシュツリーを用いてクラスタノード間でデータを比較・同期します。この処理に必要な追加メモリは、ハッシュツリーの高さ `H` によって決まります。ツリーが高いほどメモリ消費は増えますが、ハッシュが高速化され、不整合の検出・修復時間が短縮されます。

トレードオフは次のとおりです:  
  - **高い** `H`: メモリ使用量が多いが、レプリケーションが速い  
  - **低い** `H`: メモリ使用量が少ないが、レプリケーションが遅い  

:::tip マルチテナンシーにおけるメモリ管理
各テナントはシャードによってバックアップされています。そのため、テナント数が多い場合、非同期レプリケーションのメモリ消費は大きくなります。(例: 1,000 テナントでハッシュツリーの高さが 16 の場合、ノードあたり約 2 GB の追加メモリが必要ですが、高さ 20 ではノードあたり約 34 GB が必要になります)  
<br/>

メモリ消費を抑えるにはハッシュツリーの高さを下げてください。ただし、その分ハッシュ処理が遅くなり、レプリケーションも遅くなる可能性があります。
:::

クイックリファレンスとして、以下の計算式と例を利用してください。

##### メモリ計算

- **ハッシュツリー内の総ノード数:**  
  高さ `H` のハッシュツリーにおける総ノード数は次のとおりです:  
  ```
  Number of hash tree nodes = 2^(H+1) - 1 ≈ 2^(H+1)
  ```

- **必要な総メモリ (各ノードの各シャード / テナント):**  
  各ハッシュツリーノードは約 **16 バイト** のメモリを使用します。  
  ```
  Memory Required ≈ 2^(H+1) * 16 bytes
  ```

##### 例

- 高さ `16` のハッシュツリー:  
  - `総ノード数 ≈ 2^(16+1) = 131,072`  
  - `必要メモリ ≈ 131072 * 16 bytes ≈ 2,097,152 bytes (~2 MB)`  

- 高さ `20` のハッシュツリー:  
  - `総ノード数 ≈ 2^(20+1) = 2,097,152`  
  - `必要メモリ ≈ 2,097,152 * 16 bytes ≈ 33,554,432 bytes (~33 MB)`  

##### パフォーマンスの考慮: 葉ノードの数

シャード (例: テナント) 内のオブジェクトはハッシュツリーの葉ノードに分散されます。ツリーが大きいほど、各葉ノードがハッシュするデータ量が減り、比較とレプリケーションが高速化されます。

- **ハッシュツリーの葉ノード数:**  
  ```
  Number of leaves = 2^H
  ```

##### 例

- 高さ `16` のハッシュツリー:  
  - `葉ノード数 = 2^16 = 65,536`  

- 高さ `20` のハッシュツリー:  
  - `葉ノード数 = 2^20 = 1,048,576`  

:::note デフォルト設定
デフォルトのハッシュツリー高さ `16` は、メモリ消費とレプリケーション性能のバランスを取るために選定されています。クラスタノードの利用可能リソースと性能要件に応じて調整してください。
:::

### 削除解決戦略

:::info `v1.28` で追加
:::

オブジェクトが一部のレプリカに存在し、別のレプリカには存在しない場合、それは作成がまだ全レプリカに伝播されていないか、削除がまだ全レプリカに伝播されていないかのいずれかです。この 2 つを区別することが重要です。

削除解決は、非同期レプリケーションおよび repair-on-read と連携し、クラスタ全体で削除されたオブジェクトを一貫して扱います。各コレクションに対して、以下の削除解決戦略のいずれかを [設定できます](../../manage-collections/multi-node-setup.mdx#replication-settings):

- `NoAutomatedResolution`
- `DeleteOnConflict`
- `TimeBasedResolution`

削除解決戦略は可変です。[コレクション定義の更新方法](../../manage-collections/collection-operations.mdx#update-a-collection-definition) を参照してください。
#### `NoAutomatedResolution`

これはデフォルト設定であり、 `v1.28` より前の Weaviate バージョンで利用できる唯一の設定です。このモードでは、削除の競合を特別扱いしません。あるオブジェクトが一部のレプリカに存在し、他のレプリカに存在しない場合、Weaviate は欠落しているレプリカに対してオブジェクトを復元する可能性があります。

#### `DeleteOnConflict`

`deleteOnConflict` での削除競合は、常にすべてのレプリカからオブジェクトを削除することで解決されます。

そのために、Weaviate は削除リクエストを受け取った際にオブジェクトを完全に消去するのではなく、削除済みオブジェクトとしてマークします。

#### `TimeBasedResolution`

`timeBasedResolution` では、削除リクエストのタイムスタンプと、その後に行われた作成や更新などのオブジェクト操作のタイムスタンプを比較して競合を解決します。

削除リクエストのタイムスタンプが後であれば、すべてのレプリカでオブジェクトが削除されます。削除リクエストのタイムスタンプが前であれば、その後の更新がすべてのレプリカに適用されます。

例:
- オブジェクトがタイムスタンプ 100 で削除され、その後タイムスタンプ 90 で再作成された場合、再作成が優先されます  
- オブジェクトがタイムスタンプ 100 で削除され、その後タイムスタンプ 110 で再作成された場合、削除が優先されます

#### 選択の指針

- 競合を手動で管理し、最大限のコントロールを得たい場合は `NoAutomatedResolution` を使用します
- 削除を必ず反映させたい場合は `DeleteOnConflict` を使用します
- 最新の操作を優先させたい場合は `TimeBasedResolution` を使用します

### Repair-on-read

:::info v1.18 で追加
:::

読み取り整合性を `All` もしくは `Quorum` に設定している場合、リードコーディネーターは複数のレプリカから応答を受け取ります。応答が異なる場合、コーディネーターは不整合を修復しようとします。これを「repair-on-read」または「read repair」と呼びます。

| 問題 | 対応 |
| :- | :- |
| オブジェクトが一部のレプリカに存在しない | 欠落しているレプリカへオブジェクトを伝搬します |
| オブジェクトが古い | ステールなレプリカを更新します |
| オブジェクトが一部のレプリカで削除されている | エラーを返します。削除が失敗したか、オブジェクトが部分的に再作成された可能性があります |

リードリペアは、使用する読み取りおよび書き込み整合性レベルにも依存します。

| 書き込み整合性レベル | 読み取り整合性レベル | 対応 |
| :- | :- |
| `ONE` | `ALL` | すべてのノードを検証して修復を保証します |
| `QUORUM` | `QUORUM` または `ALL` | 同期の問題を修正しようとします |
| `ALL` | - | この状況は発生しないはずです。書き込みが失敗しているはずです |

修復は読み取り時にのみ発生するため、バックグラウンドの負荷は大きくありません。ただしノードが不整合状態の間、整合性レベル `ONE` の読み取りは古いデータを返す可能性があります。

## Replica movement

:::info v1.32 で追加
:::

シャードは、単一テナントコレクションではコレクションの一部、多テナントコレクションでは 1 テナント全体を表します。Weaviate では、クラスター内のソースノードからデスティネーションノードへ個々のシャードレプリカを手動で移動またはコピーできます。これにより、スケール後のクラスター再バランス、ノードの廃止、データローカリティの最適化によるパフォーマンス向上、データ可用性の向上などの運用シナリオに対応できます。

レプリカ移動はステートマシンとして動作し、プロセス全体でデータ整合性を確保します。この機能は単一テナントコレクションと多テナントコレクションの両方で利用可能です。

コレクション作成時に設定する静的なレプリケーションファクターとは異なり、レプリカ移動ではクラスター内でレプリカを移動またはコピーすることで特定のシャードに対して複製数を調整できます。コピー操作を行うと新しいレプリカが作成され、そのシャードのレプリケーションファクターが増加します。コレクションにはデフォルトのレプリケーションファクターが設定されていますが、個々のシャードはそれより高い値を持つことができます。ただし、コレクションレベルより低いレプリケーションファクターにはできません。

:::info

[`REPLICATION_ENGINE_MAX_WORKERS`](/docs/deploy/configuration/env-vars/index.md#REPLICATION_ENGINE_MAX_WORKERS) 環境変数を使用すると、レプリカ移動を並列に処理するワーカー数を調整できます。

:::

### Movement states

各レプリカ移動操作は、データ整合性と可用性を保つためのワークフローを次のステートで進行します。

- **REGISTERED**: 移動操作が開始され、Raft リーダーに記録されました。リクエストが受信され、処理待ちキューに登録されています。
- **HYDRATING**: デスティネーションノードで新しいレプリカを作成中です。データセグメントが既存のレプリカ（通常はソースレプリカ、または他の利用可能なピア）から転送されます。
- **FINALIZING**: 大量データの転送が完了し、転送中に発生した書き込みをキャッチアップしています。これにより、レプリカは最新データと完全に同期されます。進行中の書き込みが完了しターゲットノードへ複製されるまでの待機時間は、 [`REPLICA_MOVEMENT_MINIMUM_ASYNC_WAIT`](/docs/deploy/configuration/env-vars/index.md#REPLICA_MOVEMENT_MINIMUM_ASYNC_WAIT) で調整できます。
- **DEHYDRATING**: 移動 (Move) 操作の場合、新しいレプリカが準備完了した後、ソースノード上の元のレプリカを削除しています。
- **READY**: 操作が正常に完了しました。新しいレプリカは完全に同期され、トラフィックを処理できます。Move 操作の場合、ソースレプリカは削除されています。
- **CANCELLED**: 操作が完了する前にキャンセルされました。手動で中断された場合や、回復不能なエラーが発生した場合に起こります。

レプリカ移動は 2 つのモードをサポートします。

- **Move**: レプリカをあるノードから別のノードへ移動し、レプリケーションファクターは維持します  
- **Copy**: レプリカをコピーして別のノードへ追加し、そのシャードのレプリケーションファクターを 1 増やします

:::note Replication factor and quorum

シャードレプリカをコピーすると、レプリケーションファクターが偶数になる場合があります。これにより、クォーラム達成には `n/2 + 1` ノードが必要となり、以前の `n/2 + 0.5` より難しくなることがあります。たとえば `RF=3` から `RF=4` に増やすと、クォーラムに必要なノード数は 2 から 3 へ（ 67% から 75% のレプリカ）増加します。

:::

## 関連ページ
- [API リファレンス | GraphQL | Get | 整合性レベル](../../api/graphql/get.md#consistency-levels)
- <SkipLink href="/weaviate/api/rest#tag/objects">API リファレンス | REST | オブジェクト</SkipLink>

## 質問とフィードバック

import DocsFeedback from '/_includes/docs-feedback.mdx';

<DocsFeedback/>