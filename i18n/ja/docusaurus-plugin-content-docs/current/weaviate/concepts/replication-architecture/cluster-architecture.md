---
title: クラスターアーキテクチャ
sidebar_position: 3
description: "Weaviate の分散レプリケーションシステムにおけるノードの挙動とクラスター調整メカニズム。"
image: og/docs/concepts.jpg
# tags: ['architecture']
---

このページでは、 Weaviate のレプリケーション設計においてノードやクラスターがどのように動作するかを説明します。

Weaviate では、メタデータのレプリケーションとデータのレプリケーションが分離されています。メタデータには  Raft  コンセンサスアルゴリズムを使用し、データのレプリケーションにはリーダーレス設計と最終的整合性を採用しています。

## ノードディスカバリー

デフォルトでは、クラスター内の Weaviate ノードは [Hashicorp の Memberlist](https://github.com/hashicorp/memberlist) を介したゴシップ風プロトコルを使用し、ノード状態や障害シナリオを共有します。

Weaviate はクラスターとして動作する際、特に Kubernetes 上での運用に最適化されています。 [Weaviate Helm chart](/deploy/installation-guides/k8s-installation.md#weaviate-helm-chart) は `StatefulSet` とヘッドレス `Service` を利用し、ノードディスカバリーを自動的に構成します。

<details>
  <summary>ノードディスカバリーにおける FQDN</summary>

:::caution `v1.25.15` で追加、`v1.30` で削除

これは実験的機能です。ご利用の際はご注意ください。

:::

IP アドレスベースのノードディスカバリーが最適でない状況が発生することがあります。そのような場合、 `RAFT_ENABLE_FQDN_RESOLVER` と `RAFT_FQDN_RESOLVER_TLD` [環境変数](/deploy/configuration/env-vars/index.md#multi-node-instances) を設定することで、[完全修飾ドメイン名 (FQDN)](https://en.wikipedia.org/wiki/Fully_qualified_domain_name) ベースのノードディスカバリーを有効化できます。

この機能を有効にすると、 Weaviate は FQDN リゾルバーを使用してノード名を IP アドレスへ解決し、メタデータ (例:  Raft 通信) に利用します。

:::info FQDN: メタデータ変更のみ対象
この機能は、 Raft をコンセンサスメカニズムとして用いるメタデータ変更時にのみ使用されます。データの読み書き操作には影響しません。
:::

#### FQDN を使用すべき例

IP アドレスが異なるクラスター間で再利用される場合、あるクラスターのノードが別クラスターのノードを誤って検出してしまう恐れがあります。 FQDN を用いることでこれを回避できます。

また、サービス (例: Kubernetes) を利用しており、サービスの IP と実際のノード IP が異なるが、サービスがノードへの接続をプロキシしている場合にも便利です。

#### FQDN ノードディスカバリー用環境変数

`RAFT_ENABLE_FQDN_RESOLVER` は Boolean フラグで、 FQDN リゾルバーの有効／無効を切り替えます。 `true` に設定すると、 Weaviate は FQDN リゾルバーでノード名を IP アドレスへ解決します。 `false` の場合、 Memberlist ルックアップを使用します。デフォルトは `false` です。

`RAFT_FQDN_RESOLVER_TLD` は文字列で、ノード ID を IP アドレスに解決する際 `[node-id].[tld]` の形式で `[tld]` (トップレベルドメイン) を付加します。

この機能を利用するには、 `RAFT_ENABLE_FQDN_RESOLVER` を `true` に設定してください。

</details>

## メタデータレプリケーション: Raft

:::info `v1.25` で追加
:::

Weaviate はメタデータのレプリケーションに [Raft コンセンサスアルゴリズム](https://raft.github.io/) を使用し、 Hashicorp の [raft ライブラリ](https://pkg.go.dev/github.com/hashicorp/raft) で実装しています。ここでのメタデータは、コレクション定義やシャード／テナントの状態を指します。

Raft はクラスター全体でメタデータ変更の整合性を保証します。メタデータの変更はリーダーノードに転送され、リーダーが自ノードのログに適用した後、フォロワーノードへ複製します。過半数のノードが変更を承認すると、リーダーはログへコミットし、フォロワーへ通知します。フォロワーは通知を受け取り、自身のログへ変更を適用します。

この仕組みにより、少数のノード障害が発生してもクラスター全体でメタデータの一貫性が保たれます。

その結果、 Weaviate クラスターにはメタデータ変更を担当するリーダーノードが存在します。リーダーは Raft アルゴリズムによって選出され、メタデータ変更の調整を担います。

## データレプリケーション: リーダーレス

Weaviate はデータレプリケーションにリーダーレスアーキテクチャを採用しています。つまり、フォロワーノードへ複製を行う中央のリーダーやプライマリノードは存在しません。すべてのノードがクライアントからの書き込み・読み取りを受け付けられるため、高い可用性を実現します。単一障害点がないのが特徴です。リーダーレスレプリケーションは、[Dynamo 方式](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf) のデータレプリケーションとしても知られ、 [Apache Cassandra](https://cassandra.apache.org) などの OSS プロジェクトでも採用されています。

Weaviate では、クライアントの読み書き要求を適切なノードへ中継するコーディネーションパターンを使用しています。リーダーベースのデータベースとは異なり、コーディネーターノードは操作順序を強制しません。

以下の図は、 Weaviate におけるリーダーレスレプリケーション設計を示しています。 1 つのコーディネーターノードがクライアントからのトラフィックを正しいレプリカへ導きます。このノードには特別な役割はなく、単にロードバランサーからリクエストを受け取ったためにコーディネーターとなっただけです。同じデータに対する次のリクエストは、別のノードがコーディネートする可能性があります。

<p align="center"><img src="/img/docs/replication-architecture/replication-main-quorum.png" alt="レプリケーションアーキテクチャ" width="75%"/></p>

リーダーレスレプリケーションの主な利点は、耐障害性の向上です。すべてのリクエストを処理するリーダーが存在しないため、可用性が高まります。シングルリーダー設計では、書き込みは必ずリーダーで処理されるため、そのノードがダウンすると書き込みができなくなります。リーダーレス設計ではすべてのノードが書き込みを受け付けるため、マスターノード障害のリスクがありません。

高い可用性の反面、リーダーレスデータベースは整合性が低下しがちです。リーダーノードが存在しないため、ノード間で一時的にデータが古くなる可能性があります。リーダーレスデータベースは最終的整合性に向かう傾向があります。 Weaviate では整合性を [調整可能](./consistency.md) ですが、その分可用性を犠牲にします。

## レプリケーションファクター

import RaftRFChangeWarning from '/_includes/1-25-replication-factor.mdx';

<RaftRFChangeWarning/>

Weaviate では、データのレプリケーションはコレクション単位で有効化および制御されます。そのため、コレクションごとに異なるレプリケーションファクターを設定できます。

レプリケーションファクター (RF または n) は、分散環境でデータが何個複製されるかを決定します。レプリケーションファクターが 1 の場合、各データエントリは 1 つしか存在せず、レプリケーションは行われません。レプリケーションファクターが 2 の場合、各データエントリは 2 つの異なるノード (レプリカ) に複製されます。当然ながら、レプリケーションファクターはノード数を超えることはできません。クラスター内のどのノードもコーディネーターノードとして機能し、クエリを適切なターゲットノードへ導けます。

レプリケーションファクター 3 は、パフォーマンスと耐障害性のバランスが取れているため一般的に使用されます。奇数ノード数が推奨されるのは、競合解決が容易になるためです。 3 ノード構成では、 2 ノードでクォーラムを達成でき、耐障害性は 1 ノードとなります。一方 2 ノード構成では、コンセンサス達成中にノード障害を許容できません。 4 ノード構成では、 3 ノードが必要になります。したがって、 3 ノード構成は 2 ノードや 4 ノードよりコスト対耐障害性の比率が優れています。

<p align="center"><img src="/img/docs/replication-architecture/replication-factor.png" alt="レプリケーションファクター" width="75%"/></p>

## 書き込み操作

書き込み操作では、クライアントのリクエストがクラスター内の任意のノードに送信されます。最初にリクエストを受け取ったノードがコーディネーターノードとして割り当てられます。コーディネーターノードはリクエストをあらかじめ定義された複数のレプリカノードへ送信し、結果をクライアントへ返します。そのため、クラスター内のどのノードもコーディネーターノードになり得ます。クライアントはこのコーディネーターノードとだけ直接通信します。結果を返す前に、コーディネーターノードは設定に応じた数の書き込み ACK を待ちます。 Weaviate がいくつの ACK を待つかは [整合性設定](./consistency.md) に依存します。

**手順**
1. クライアントがデータを任意のノードに送信し、そのノードがコーディネーターノードとなる  
2. コーディネーターノードがデータを複数のレプリカノードへ送信する  
3. コーディネーターノードは、クラスターノードの指定割合 (ここでは `x`) から ACK を待機する。 v1.18 以降、 `x` は [設定可能](./consistency.md) で、デフォルトは `ALL`  
4. `x` 件の ACK を受け取ると、書き込みは成功する  

例として、クラスターサイズ 3・レプリケーションファクター 3 の場合を考えます。この場合、すべてのノードがデータのコピーを保持します。クライアントが新しいデータを送信すると、 3 ノードすべてに複製されます。

<p align="center"><img src="/img/docs/replication-architecture/replication-rf3-size3.png" alt="クラスターサイズ 3 におけるレプリケーションファクター 3" width="75%"/></p>

クラスターサイズ 8・レプリケーションファクター 3 の場合、書き込みは 8 ノードすべてではなく、該当レプリカを保持する 3 ノードのみに送信されます。コーディネーターノードがどのノードに書き込むかを決定します。どのノードがどのコレクション (およびシャード) を保持するかは Weaviate のセットアップで決定され、各ノード (すなわち各コーディネーターノード) が把握しています。どこに複製されるかは決定論的であり、すべてのノードがどのシャードにどのデータが入るかを認識しています。

<p align="center"><img src="/img/docs/replication-architecture/replication-rf3-size8.png" alt="クラスターサイズ 8 におけるレプリケーションファクター 3" width="75%"/></p>

## 読み取り操作

読み取り操作もコーディネーターノードによって調整され、クエリはデータを保持している適切なノードへ送信されます。一部のノードが古い (スタイル) データを保持している可能性があるため、読み取りクライアントは受け取ったデータのうち最新のものを選択してユーザーへ返します。

**手順**
1. クライアントが Weaviate にクエリを送信し、最初に受信したノードがコーディネーターノードとなる  
2. コーディネーターノードがクエリを複数のレプリカノードへ送信する  
3. コーディネーターノードは x ノードからのレスポンスを待つ。 *x は [設定可能](./consistency.md) (`ALL`, `QUORUM`, `ONE`、 v1.18 以降。 Get-Object-By-ID 形式のリクエストは v1.17 から調整可能)*  
4. コーディネーターノードがメタデータ (例: タイムスタンプ、ID、バージョン番号) を用いて競合データを解決する  
5. コーディネーターノードが最新のデータをクライアントへ返す  

クラスターサイズ 3、レプリケーションファクター 3 の場合、すべてのノードがクエリを処理できます。整合性レベルが、何ノードへクエリを送信するかを決定します。

クラスターサイズ 10、レプリケーションファクター 3 の場合、該当データ (コレクション) を保持する 3 ノードがクエリを処理し、コーディネーターノードがそれを調整します。クライアントは x (整合性レベル) ノードから応答を受け取るまで待機します。
## 質問とフィードバック

import DocsFeedback from '/_includes/docs-feedback.mdx';

<DocsFeedback/>