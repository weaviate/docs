---
title: ランドスケープの把握
description: 広大で複雑な AI モデルの世界をナビゲートする方法
---

## <i class="fa-solid fa-chalkboard-user"></i> アクセスモードの選択

アプリケーションに AI モデルを導入する際、商用の推論サービスプロバイダーを利用するかローカル推論を行うかを選ぶことで、選択肢を絞り込むことができます。

それぞれのアプローチには、ユースケースに応じて考慮すべき明確な利点とトレードオフがあります。

### <i class="fa-solid fa-chalkboard"></i> 推論サービスプロバイダー

例: AWS、Cohere、Google、Microsoft Azure、OpenAI など

**利点:**

- インフラ管理やハードウェア投資が不要
- プロバイダー独自の専有モデルへアクセス可能  
  - 定期的なモデル更新と改善
- ワークロードの変動に応じた自動スケーリング

**欠点:**

- ローカルまたはセルフマネージド推論よりコストが高い傾向
- データが環境外に出るためプライバシー面の考慮が必要
- サードパーティサービスの稼働状況に依存  
  - プロバイダーがサービスを停止した場合のリスク
- カスタマイズの自由度が限定的

### <i class="fa-solid fa-chalkboard"></i> ローカル推論

例: Hugging Face Transformers / Accelerate、llama.cpp、Ollama、PyTorch Serve、TensorFlow Serving

**利点:**

- データ送受信を自分で管理可能
- 推論にインターネット接続が不要
- 独自に学習させたカスタムモデルのデプロイが可能

**欠点:**

- ハードウェア調達の初期費用が高い
- 最新で最強力なモデルにアクセスしにくい
- モデルの更新・保守を自分で担う必要
- 使用可能なハードウェアによる性能制約の可能性

### <i class="fa-solid fa-chalkboard"></i> 判断基準

選択の際は、次の質問を検討してください。

1. **データの機密性**: 機密情報や規制対象のデータを扱いますか？ データを外部に出せない場合、利用可能な推論プロバイダーやローカルモデルが限定されます。
2. **推論量**: 1 日 / 1 か月あたりどの程度のリクエストを処理しますか？ 多くの場合、ローカルモデルの初期コストが負担となる可能性があります。
3. **レイテンシ要件**: モデル応答の時間制約はどの程度厳しいですか？ 厳しい場合、小規模ユーザーがローカルモデルを使うことは難しいかもしれません。
4. **予算制約**: 優先するのは初期費用の削減ですか、それとも長期的なコスト最適化ですか？
5. **技術リソース**: ローカルモデルのデプロイや学習を管理する能力がありますか？

万能な解決策は存在しません。

しかし、まずは商用の推論サービスプロバイダーを利用すると、導入時のハードルが低くなる場合があります。

## <i class="fa-solid fa-chalkboard-user"></i> モデルカードの読み方

モデル「カード」は、一般製品における製品ラベルや仕様書と同じように、 AI モデルについての情報を提供するものです。

モデルカードは、モデルプロバイダーがモデルの概要と最適な利用方法を理解してもらうために提供します。

**モデルカードの例**

モデルカードには複数の形式があります。以下は埋め込みモデルのカード例です。

- [https://huggingface.co/Cohere/Cohere-embed-english-v3.0](https://huggingface.co/Cohere/Cohere-embed-english-v3.0)
- [https://docs.cohere.com/v2/docs/cohere-embed](https://docs.cohere.com/v2/docs/cohere-embed)
- [https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0](https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0)

生成 AI モデルのカード例は次のとおりです。

- [https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)
- [https://ollama.com/library/llama3.3:70b](https://ollama.com/library/llama3.3:70b)
- [https://ai.google.dev/gemma/docs/core/model_card_3](https://ai.google.dev/gemma/docs/core/model_card_3)
- [https://build.nvidia.com/microsoft/phi-4-multimodal-instruct/modelcard](https://build.nvidia.com/microsoft/phi-4-multimodal-instruct/modelcard)

この少数の例だけでも、カードはホスティングサイトやモデルプロバイダーによって内容が大きく異なることが分かります。特に初めての場合、この情報量は圧倒されるかもしれません。

import ModelCards from './_img/model_cards.png';

<img src={ModelCards} alt="Model Cards" />

詳細は後続のコースで扱いますが、ここでは注目すべき主要パラメーターを紹介します。

1. **基本情報**  
   - モデル名とバージョン  
   - モデルタイプ（生成、埋め込み など）  
   - モデルサイズ（パラメータ数）とアーキテクチャ  
   - 学習データの概要  
   - API 経由でアクセスする場合のコスト、またはハードウェア要件  
2. **技術仕様**  
   - 埋め込みモデルの場合の次元数  
   - 生成モデルの場合のコンテキスト長  
   - 対応言語やモダリティ  
3. **性能指標**  
   - ベンチマーク結果  
   - 既知の強みと制限  
   - 下流タスクでの性能  
4. **使用情報**  
   - 想定ユースケース  
   - 実装ガイドラインやコードスニペット  
5. **法的・倫理的考慮事項**  
   - ライセンス形態と使用制限  
   - バイアスや公平性に関する潜在的課題  

これらの項目を確認するだけでも、自分のニーズに合ったモデルを見極めるのに大いに役立ちます。

他の情報がない場合は、まず次の観点でフィルタリングすると良いでしょう。

- 適切なモデルタイプ
- モダリティ、言語、コンテキスト長または次元数の適合性
- モデルへのアクセス方法（推論サービスプロバイダー / ローカル推論）
- ライセンスが要件を満たしているか

そのうえで、信頼できるモデルプロバイダーのモデルやベンチマーク性能を基に候補を選定します。

最終的には、モデルおよびアプリケーションの性能を自分で評価したい場合もあるでしょう。

しかし、これらのシンプルなヒューリスティクスを活用することで、優れたベースラインモデル、または複数のベースラインモデルを選ぶまでの道のりが大きく短縮されます。
## 質問とフィードバック

import DocsFeedback from '/_includes/docs-feedback.mdx';

<DocsFeedback/>