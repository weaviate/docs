---
title: 埋め込みモデル
description: 埋め込みモデルの内部を探る
---

import ThemedImage from '@theme/ThemedImage';

:::info Embedding models and Weaviate

埋め込みはベクトル データベースであるWeaviateにとって不可欠で、ベクトル／セマンティック検索を実現します。Weaviateのようなベクトル データベースを使えば、これらの埋め込みを数百万、あるいは数十億件でも簡単に保存・検索できます。

:::

生成モデルが AI 世界のスターだとすれば、埋め込みモデルは配管のような存在です。華やかさはありませんが、インフラを支える重要な部品です。

埋め込みモデルとは何か、そしてなぜそれほど重要なのかを見ていきましょう。

## <i class="fa-solid fa-chalkboard-user"></i> 埋め込みモデルの仕組み

埋め込みモデルは、与えられた入力の「意味」を捉えることを目指します。生成された埋め込みは、その後、分類・クラスタリング、あるいは最も一般的には情報検索などのタスクに利用できます。

よく使われる埋め込みモデルには、Cohere の `embed-multilingual-v3.0`、OpenAI の `text-embedding-3-large`、Snowflake の `snowflake-arctic-embed-l-v2.0` などがあります。

これらの名前は総じて汎用的で説明的です。派手さもないため、あまり知られていないかもしれません。

しかし、生成モデルに劣らず興味深い存在であり、多くの共通点もあります。たとえば、どちらもテキスト入力をトークナイザーで数値形式へ変換します。大きな違いは、生成モデルがトークンを 1 つずつ出力するのに対し、埋め込みモデルは固定長（あるいは固定形状）の数列を一度に出力する点です。

import NNEmbeddingModels from './_img/nn_explained_50_embedding_models.png';  
import NNEmbeddingModelsDark from './_img/nn_explained_50_embedding_models_dark.png';  

<ThemedImage
  alt="ニューラルネットワークの基本図"
  sources={{
    light: NNEmbeddingModels,
    dark: NNEmbeddingModelsDark,
  }}
  width="400"
/>

意味を数値で表現するという概念は、最初はピンとこないかもしれません。ここで一歩戻り、よりシンプルな例を見てみましょう。

### <i class="fa-solid fa-chalkboard-user"></i> 類推：色のエンコーディング

このアプローチの類推として、 RGB や CMYK、 HSL など、色を数値で表現する方法が挙げられます。

いずれの方式でも、任意の色を数値の列として表現できます。たとえばウェブ公式の「red」を例に取ると、 RGB では `[255, 0, 0]`、 CMYK では `[0, 100, 100, 0]`、 HSL では `[0, 100%, 50%]` となります。

つまり、それぞれの方式は、色を数列として表現するための標準化された方法です。

埋め込みモデルも同様に機能します。たとえば、「You’re a wizard, Harry.」というフレーズは次のように表現されるかもしれません。

`[0.021, -0.103, 0.036, 0.088, -0.022, ..., 0.056]`

実際のシーケンス長は、 256、 1024、 1536 など、かなり大きいことが一般的です。

もちろん、最新の埋め込みモデルは、色を RGB 値に変換するアルゴリズムよりはるかに複雑です。しかし原理は同じで、どの方式も入力を一貫して数列に変換します。

では、これがどのように役立つのでしょうか。実はさまざまなタスクで利用できます。

## <i class="fa-solid fa-chalkboard-user"></i> 埋め込みモデルを使う理由

先ほど、埋め込みモデルを色を数値化する RGB のような体系になぞらえました。埋め込みモデルの最大の利点も同様で、元の対象同士を意味のある形で比較できる点にあります。

 RGB に戻ると、「crimson」は `[220, 20, 60]` です。これが red の `[255, 0, 0]` とかなり近く、一方で「aqua」の `[0, 255, 255]` とは大きく異なることが分かります。

|  | R | G | B |
| --- | --- | --- | --- |
| Red | 255 | 0 | 0 |
| Crimson | 220 | 20 | 60 |
| Aqua | 0 | 255 | 255 |

実際には、類似度を 1 つの数値として定量化できます。そこでよく使用される指標が「コサイン類似度」です。 Python での実装例を示します。

```python
import numpy as np

def cosine_similarity(a: list, b: list) -> float:
    # Calculate the cosine similarity between two input lists
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
```

比較結果を表にまとめると、これを類似度行列と呼びます。ここでは各セルが、対応する行と列の類似度を表しています。

|  | Red | Crimson | Aqua |
| --- | --- | --- | --- |
| Red | 1 | 0.961 | 0 |
| Crimson | 0.961 | 1 | 0.247 |
| Aqua | 0 | 0.247 | 1 |

このデモからの重要なポイントは、この方式を使えば色同士の類似度を比較できるということです。言い換えれば、ある色に最も似ている色を多数の候補の中から特定できるようになります。

## <i class="fa-solid fa-chalkboard-user"></i> 埋め込みモデルの応用

埋め込みモデルの話に戻ると、その価値は、与えられたオブジェクトに対して最も意味が近いオブジェクトを集合の中から見つけ出せる点にあります。

次の 3 つのテキストを見てみましょう。

```
1. "You're not wizened, Harry."
2. "Harry can wield magic."
3. "Ron is not a great driver."
```

これらのうち、`"You're a wizard, Harry."` に最も近いのはどれでしょうか。

おそらく多くの人が 2 と答えるでしょう。しかしその理由は？ そしてプログラムにも同じ答えを出させるには？ 2 にはクエリと重複する語が 1 つしか含まれていない点に注目してください。

これこそ埋め込みが可能にするタスクです。埋め込みとコサイン類似度を使うと、次のような結果になります。

|  | 順位 | コサイン距離 |
| --- | --- | --- |
| Harry can wield magic. | 1 | 0.238 |
| You're not wizened, Harry. | 2 | 0.274 |
| Ron is not a great driver. | 3 | 0.803 |

この類似度の概念は、セマンティック検索で利用されます。最新の AI システムでは、セマンティック検索は 検索拡張生成 ( RAG ) の重要な構成要素となっており、生成システムに正確で最新のコンテキストを提供して補完します。

埋め込みの応用範囲はさらに広がっています。レコメンダー、クラスタリング、分類など、他の AI システムでも広く利用されています。

:::tip Advanced topics

本節では、テキスト入力からベクトル 埋め込みを生成するテキスト埋め込みモデルに関する話題が中心でした。  
<br/>

生成モデルと同様、埋め込みモデルの世界も非常に大きく興味深いものです。マルチモーダル埋め込みモデルは複数の入力タイプを受け取り、同じ空間で互換性のある埋め込みを生成できます。また、近年の埋め込みには、多ベクトル 埋め込み（例: ColBERT）や可変長埋め込みなど、さまざまな形式があります。  
<br/>

これらについては、特定のモダリティやモデル選定をより深く掘り下げる際に、改めて取り上げる予定です。

:::
## 質問とフィードバック

import DocsFeedback from '/_includes/docs-feedback.mdx';

<DocsFeedback/>