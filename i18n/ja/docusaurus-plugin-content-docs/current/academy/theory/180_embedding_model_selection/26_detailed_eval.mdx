---
title: 詳細評価
description: モデルの候補を絞り込んだら、詳細に評価できます。
---

import ThemedImage from '@theme/ThemedImage';

## <i class="fa-solid fa-chalkboard-user"></i> 概要

ニーズを明確にし、候補モデルのショートリストを作成したら、次は徹底的な評価を行えます。この段階の目的は、ご自身のユースケースに適した埋め込みモデルを選定するための具体的な根拠を集めることです。  

埋め込みモデルを評価する際には、次の 2 種類の評価を区別することが重要です。  

1. **モデル評価**: ベクトル埋め込みモデル自体を直接的な指標で評価します。通常、標準ベンチマークや特定のユースケースに合わせて設計したカスタムタスクで性能を測定します。  
2. **ダウンストリーム評価**: エンベディングモデルが RAG パイプラインやレコメンデーションエンジンなど、より大きなシステムやアプリケーション内でどのように機能するかを確認します。  

どちらの評価も重要ですが、目的は異なります。モデル評価は各モデルの本来の能力を理解するのに役立ちます。一方、ダウンストリーム評価は、それらの能力がシステムとしての実際のアプリケーション性能にどのように反映されるかを示します。  

このガイドでは、まずモデル評価に焦点を当てます。ダウンストリーム、つまりシステムレベルの評価については、後ほど扱います。  

まずは標準ベンチマークを用いたモデル評価から始めましょう。  

## <i class="fa-solid fa-chalkboard-user"></i> 標準ベンチマーク

標準ベンチマークは、詳細なモデル評価を始める際の良い出発点になります。専門家によって作成されており、多くの場合、最小限の労力で結果を参照できます。  

ベンチマーク結果を確認する際は、ご自身のユースケースに最も関連するタスクと指標に注目してください。  

前述のとおり、 Massive Text Embedding Benchmark ( MTEB ) は業界標準として広く使われており、出発点として優れています。  

ここでは、詳細な評価のためにベンチマーク結果をどのように解釈するかを掘り下げます。 MTEB は 100 以上の個別タスクで構成されており、タスクタイプごとにモデル性能を示します。テキストモデルの一般的なタスクタイプは次のとおりです。  

- **検索 ( Retrieval )**: クエリに基づいて関連ドキュメントを見つける  
- **分類 ( Classification )**: テキストをあらかじめ定義されたクラスに分類する  
- **クラスタリング ( Clustering )**: 類似したテキストをグループ化する  
- **再ランキング ( Reranking )**: 関連性で結果を並べ替える  
- **セマンティック・テキスト類似度 ( STS )**: 2 つのテキストが意味的にどれだけ類似しているかを測定する  

多くの AI アプリケーションで最も一般的なタスクタイプは検索かもしれません。実際、すでに MTEB の検索スコアについては触れました。しかし、 MTEB では個別タスクのスコアも取得できるため、総合スコア以上に掘り下げる価値があります。  

### <i class="fa-solid fa-chalkboard"></i> MTEB スコアの詳細確認

 MTEB における各タスクタイプのスコアは、複数タスクのスコアを組み合わせて構成されています。例えば 2025 年 4 月時点で、検索スコアは 17 種類のタスクベンチマークのスコアで成り立っています。  

以下の図では、いくつかのモデル性能をスコアの可視化で比較します。これらの図は、読みやすさのために多言語タスクセットを 2 つのサブセットに分割しています。  

この最初の画像は、ニュースや百科事典など一般ドメインのデータを使用するタスクを含みます。  

<img
    src={require('./_img/mteb-benchmarks-by-task-general.png').default}
    alt="MTEB ベンチマーク（一般タスク別）"
/>

次の画像は、より専門的なドメインデータを使用するタスクを含みます。法務、医療、プログラミング、政府データなど、さまざまな分野をカバーしています。  

<img
    src={require('./_img/mteb-benchmarks-by-task-specialized.png').default}
    alt="MTEB ベンチマーク（専門タスク別）"
/>

チャートから、表の上位に位置する `gemini-embedding-exp-03-07` などのエンベディングモデルは、他モデルと比べて幅広いタスクで良好な性能を示していることがわかります。しかし、これだけでは全体像を語れません。モデルによっては平均スコア以上に特定タスクで高い性能を発揮する場合があります。  

たとえば、 `snowflake-arctic-embed` モデルは、長いテキスト中に埋もれた特定の語句のリコールを試す `LEMBPasskeyRetrieval` タスクで非常に優れた結果を出しています。また、 Cohere の `Cohere-embed-multilingual-v3.0` は、多言語性が高い MIRACL タスクで良好な性能を示しています。  

興味深いことに、 MTEB の多言語タスクセットを見ていても、英語のみ（または英語が大部分）で構成されたタスクが含まれている場合があります。  

<img
    src={require('./_img/mteb-tasks-example.png').default}
    alt="MTEB 多言語タスク例"
/>

そのため、各タスクがニーズにどれだけ対応しているかに基づき、タスクスコアを組み合わせて独自の指標を導出すると役立つかもしれません。  

考慮すべきポイントは次のとおりです。  

1. **タスクの関連性**: タスクはユースケースに合致していますか？  
2. **データ分布**: データはドメインを代表していますか？  
3. **指標の関連性**: 報告されている指標は要件に合っていますか？  
4. **最新性**: 結果は現在のモデル性能を反映するほど新しいですか？  
5. **公平性**: すべてのモデルが同等の条件で評価されていますか？  

たとえば、データが複数言語を確実に含む場合は、多言語データセットの重みを単言語データセットより高く設定するかもしれません。法務、医療、プログラミングなど、ドメイン固有データでも同様です。  

その結果得られるスコアは公式の総合スコアとは異なるかもしれませんが、特定のユースケースにはより適切な指標となり得ます。  

### <i class="fa-solid fa-chalkboard"></i> 標準ベンチマークの限界

これらのサードパーティベンチマークは非常に有用ですが、いくつかの限界があることも念頭に置く必要があります。主な 2 つの限界はデータ漏洩とニーズとの相関性です。  

**データ漏洩 ( Data leakage )**  

ベンチマークが公開されているため、一部のベンチマークデータがモデルの学習データに含まれてしまうリスクがあります。特に、大量のデータが学習に使用される現在では、この問題が起こりやすくなっています。  
その結果、モデルが「記憶」した内容を答えてしまい、ベンチマーク結果がモデル本来の性能を正しく表さない可能性があります。  

**ニーズとの相関性**  

もう 1 つの限界は、標準ベンチマークが必ずしも自分のニーズを正確に反映していない点です。ニーズに最も近いベンチマークを選ぶことはできますが、タスク、データ分布、指標が完全に一致することは稀です。  

**緩和策 ( Mitigation )**  

そのため、標準ベンチマークの結果はあくまで参考程度に留めることが大切です。追加の手掛かりとして、自分でベンチマークを作成して試す方法が有効です。次のセクションで詳しく見ていきましょう。  

## <i class="fa-solid fa-chalkboard-user"></i> モデル評価: カスタムベンチマーク

標準ベンチマークが貴重な参考情報を提供してくれる一方で、独自の評価を作成することは、その限界を補う優れた手段になります。  

 MTEB のような大規模ベンチマークを考えると、自分でベンチマークを走らせるのは難しく思えるかもしれません。しかし、以下のステップを踏めば、それほど難しくありません。  

### <i class="fa-solid fa-chalkboard"></i> ベンチマーク目標の設定

ここまでで、知識ギャップや具体的なタスクが把握できているはずです。たとえば、次のような疑問があるかもしれません。  

- どのモデルが、主に英語・フランス語・韓国語で書かれたコーヒーに関する顧客レビューを最適に検索できるか？  
- Python と Golang のバックエンド Web コードチャンク、およびそれに関連するドキュメントスニペットを対象としたコード検索で、どのモデルが一貫して優れた性能を示すか？  

カスタムベンチマークは、こうした具体的な疑問を解決できるように設計する必要があります。
### <i class="fa-solid fa-chalkboard"></i> 使用指標の決定

全体目標が定義されると、それに対応する指標を決定できます。

たとえば、検索性能は一般的に precision、recall、 MAP、 MRR、 NDCG のいずれか、または複数で測定されます。

これらはそれぞれ検索性能の異なる側面を測定しますが、 NDCG を使うのがよい出発点になります。

NDCG は関連性に基づいてアイテムを正しく並べ替える能力を測定します。クエリと、そのクエリに対してランク付けされたデータセットが与えられると、 NDCG は上位にランク付けされたアイテムが検索結果でも上位に配置されているほど高く評価します。

スコアは 0 から 1 で表され、0 はランク付けされたアイテムがまったく取得できなかった状態、1 はすべてのトップランクアイテムを取得し、正しい順序で並べ替えられた状態を意味します。

### <i class="fa-solid fa-chalkboard"></i> ベンチマーク用データセットの作成

適切なデータセットは、ベンチマークを有意義にするうえで不可欠です。既存のデータセットが使える場合もありますが、ベンチマークの目的や指標に合わせてデータセットを構築・再構成することが一般的です。

データセットは次の点を目指すべきです。

- 検索タスクを反映する  
- タスクの難易度を反映する  
- データ分布を捉える  
- 十分なボリュームを含む  

これがプロセスで最も時間がかかる部分かもしれません。しかし、実用的なアプローチを取れば管理しやすくなります。オブジェクトが 20 件ほど、クエリが数件のベンチマークでも意味のある結果を得られる場合があります。

### <i class="fa-solid fa-chalkboard"></i> ベンチマークの実行

ここで、候補モデルを使ってベンチマークを実行します。

多くの科学的プロジェクトと同様に、再現性と一貫性が鍵となります。また、後から新しいモデルを評価したり、ニーズに関する知見が更新されたりした際に戻ってくる可能性があることも念頭に置きましょう。

プログラミング上は、埋め込み生成、データセット読み込み、指標評価、結果表示などの要素をモジュール化するとよいでしょう。

### <i class="fa-solid fa-chalkboard"></i> 結果の評価

ベンチマークを実行したら、定量的（例： NDCG@k の数値）および定性的（例：どのオブジェクトがどこで取得されたか）手段で結果を評価することが重要です。

定量的結果は、モデルを並べ替えるなどの際に使用できる決定的なランキングを提供します。ただし、これはデータセット構成や使用している指標など多くの要因の影響を受けます。

一方、定性的結果はより重要な洞察を与えてくれる場合があります。たとえば、ある埋め込みモデルが次のような傾向を示すことがあります。

- 非常に関連性が高いが短いテキストを取得できず、長いテキストを優先する  
- 肯定的な文では性能が高いが、否定文では性能が低い  
- ドメイン固有の専門用語が苦手  
- 英語と中国語（普通話）では良好だが、あなたのデータにとって重要なハンガリー語ではうまく機能しない  

これらの洞察の多くは、ドメインに精通した人や構築中のシステムの文脈を理解している人にしか発見できない場合があります。そのため、定性的評価は極めて重要です。

## 質問とフィードバック

import DocsFeedback from '/_includes/docs-feedback.mdx';

<DocsFeedback/>